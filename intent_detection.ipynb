{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Detection Notebook\n",
    "\n",
    "This notebook presents my approach to developing a complete end-to-end NLU model capable of predicting user intent.\n",
    "\n",
    "The code explores various techniques for intent detection, including zero-shot classification with traditional NLU models, embedding-based methods, and LLM-based approaches.\n",
    "\n",
    "The goal is to evaluate the performance of these methods on the training dataset provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: huggingface in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (0.31.2)\n",
      "Requirement already satisfied: hf_xet in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: open-intent-classifier in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (0.0.8)\n",
      "Requirement already satisfied: openai in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (1.75.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.78.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (0.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (1.6.1)\n",
      "Requirement already satisfied: sentence-transformers in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (4.1.0)\n",
      "Requirement already satisfied: pandas in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (1.1.0)\n",
      "Requirement already satisfied: dspy in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (2.6.23)\n",
      "Requirement already satisfied: accelerate in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from open-intent-classifier) (1.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from accelerate->open-intent-classifier) (7.0.0)\n",
      "Requirement already satisfied: backoff>=2.2 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (1.5.0)\n",
      "Requirement already satisfied: ujson>=5.8.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (5.10.0)\n",
      "Requirement already satisfied: datasets>=2.14.6 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (3.6.0)\n",
      "Requirement already satisfied: optuna>=3.4.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (4.3.0)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (0.1.6)\n",
      "Requirement already satisfied: litellm>=1.60.3 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (1.69.1)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (0.44.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (9.1.2)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (3.1.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from dspy->open-intent-classifier) (14.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from pandas->open-intent-classifier) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from pandas->open-intent-classifier) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from pandas->open-intent-classifier) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from scikit-learn->open-intent-classifier) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from scikit-learn->open-intent-classifier) (3.6.0)\n",
      "Requirement already satisfied: Pillow in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from sentence-transformers->open-intent-classifier) (11.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from datasets>=2.14.6->dspy->open-intent-classifier) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from datasets>=2.14.6->dspy->open-intent-classifier) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from datasets>=2.14.6->dspy->open-intent-classifier) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from datasets>=2.14.6->dspy->open-intent-classifier) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from litellm>=1.60.3->dspy->open-intent-classifier) (3.11.18)\n",
      "Requirement already satisfied: click in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from litellm>=1.60.3->dspy->open-intent-classifier) (8.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from litellm>=1.60.3->dspy->open-intent-classifier) (8.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from litellm>=1.60.3->dspy->open-intent-classifier) (4.23.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from litellm>=1.60.3->dspy->open-intent-classifier) (0.9.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from optuna>=3.4.0->dspy->open-intent-classifier) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from optuna>=3.4.0->dspy->open-intent-classifier) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from optuna>=3.4.0->dspy->open-intent-classifier) (2.0.40)\n",
      "Requirement already satisfied: six>=1.5 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->open-intent-classifier) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from rich>=13.7.1->dspy->open-intent-classifier) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from rich>=13.7.1->dspy->open-intent-classifier) (2.19.1)\n",
      "Requirement already satisfied: Mako in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy->open-intent-classifier) (1.3.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from aiohttp->litellm>=1.60.3->dspy->open-intent-classifier) (1.20.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.60.3->dspy->open-intent-classifier) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy->open-intent-classifier) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy->open-intent-classifier) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy->open-intent-classifier) (0.24.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy->open-intent-classifier) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch transformers huggingface huggingface_hub hf_xet open-intent-classifier openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Data\n",
    "\n",
    "Loading the dataset to analyze the available intents and their corresponding utterances.\n",
    "\n",
    "The dataset contains user utterances labeled with their corresponding intents.\n",
    "\n",
    "This exploration helps me understand the data structure and content, as this will serve as the gold standard for evaluating the intent detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Intents: 21\n",
      "\n",
      "Available Intent Labels:\n",
      "----------------------\n",
      "- 100_NIGHT_TRIAL_OFFER\n",
      "- ABOUT_SOF_MATTRESS\n",
      "- CANCEL_ORDER\n",
      "- CHECK_PINCODE\n",
      "- COD\n",
      "- COMPARISON\n",
      "- DELAY_IN_DELIVERY\n",
      "- DISTRIBUTORS\n",
      "- EMI\n",
      "- ERGO_FEATURES\n",
      "- LEAD_GEN\n",
      "- MATTRESS_COST\n",
      "- OFFERS\n",
      "- ORDER_STATUS\n",
      "- ORTHO_FEATURES\n",
      "- PILLOWS\n",
      "- PRODUCT_VARIANTS\n",
      "- RETURN_EXCHANGE\n",
      "- SIZE_CUSTOMIZATION\n",
      "- WARRANTY\n",
      "- WHAT_SIZE_TO_ORDER\n",
      "\n",
      "Example utterances for each label:\n",
      "--------------------------------\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER:\n",
      "  - How does the 100 night trial work\n",
      "  - What is the 100-night offer\n",
      "  - Trial details\n",
      "\n",
      "ABOUT_SOF_MATTRESS:\n",
      "  - How is SOF different from other mattress brands\n",
      "  - Why SOF mattress\n",
      "  - About SOF Mattress\n",
      "\n",
      "CANCEL_ORDER:\n",
      "  - I want to cancel my order\n",
      "  - How can I cancel my order\n",
      "  - Cancel order\n",
      "\n",
      "CHECK_PINCODE:\n",
      "  - Do you deliver to my pincode\n",
      "  - Check pincode\n",
      "  - Is delivery possible on this pincode\n",
      "\n",
      "COD:\n",
      "  - COD option is availble?\n",
      "  - Do you offer COD to my pincode?\n",
      "  - Can I do COD?\n",
      "\n",
      "COMPARISON:\n",
      "  - What is the difference between the Ergo & Ortho variants\n",
      "  - Difference between Ergo & Ortho Mattress\n",
      "  - Difference between the products\n",
      "\n",
      "DELAY_IN_DELIVERY:\n",
      "  - It's been a month\n",
      "  - Why so long?\n",
      "  - I did not receive my order yet\n",
      "\n",
      "DISTRIBUTORS:\n",
      "  - Do you have any showrooms in Delhi state\n",
      "  - Do you have any distributors in Mumbai city\n",
      "  - Do you have any retailers in Pune city\n",
      "\n",
      "EMI:\n",
      "  - You guys provide EMI option?\n",
      "  - Do you offer Zero Percent EMI payment options?\n",
      "  - 0% EMI.\n",
      "\n",
      "ERGO_FEATURES:\n",
      "  - What are the key features of the SOF Ergo mattress\n",
      "  - Features of Ergo mattress\n",
      "  - SOF ergo\n",
      "\n",
      "LEAD_GEN:\n",
      "  - Get in Touch\n",
      "  - Want to talk to an live agent\n",
      "  -  Please call me\n",
      "\n",
      "MATTRESS_COST:\n",
      "  - Price of mattress\n",
      "  - Mattress cost\n",
      "  - Cost of mattress\n",
      "\n",
      "OFFERS:\n",
      "  - Offers\n",
      "  - What are the available offers\n",
      "  - Give me some discount\n",
      "\n",
      "ORDER_STATUS:\n",
      "  - Order Status\n",
      "  - What is my order status?\n",
      "  - Order related\n",
      "\n",
      "ORTHO_FEATURES:\n",
      "  - Features of Ortho mattress\n",
      "  - What are the key features of the SOF Ortho mattress\n",
      "  - SOF ortho\n",
      "\n",
      "PILLOWS:\n",
      "  - Can I get pillows?\n",
      "  - Do you sell pillows?\n",
      "  - Pillows\n",
      "\n",
      "PRODUCT_VARIANTS:\n",
      "  - What are the product variants\n",
      "  - Product Variants\n",
      "  - Help me with different products\n",
      "\n",
      "RETURN_EXCHANGE:\n",
      "  - Need my money back\n",
      "  - I want refund\n",
      "  - Refund\n",
      "\n",
      "SIZE_CUSTOMIZATION:\n",
      "  - I want to change the size of the mattress.\n",
      "  - Need some help in changing size of the mattress\n",
      "  - How can I order a custom sized mattress\n",
      "\n",
      "WARRANTY:\n",
      "  - What is the warranty period?\n",
      "  - Warranty\n",
      "  - Does mattress cover is included in warranty\n",
      "\n",
      "WHAT_SIZE_TO_ORDER:\n",
      "  - Can you help with the size?\n",
      "  - How do I know what size to order?\n",
      "  - How do I know the size of my bed?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/amogh/Documents/amogh/personal/Tifin_Test_1/data/sofmattress_train.csv')\n",
    "\n",
    "# Create a dictionary to store utterances by label\n",
    "intent_data = defaultdict(list)\n",
    "\n",
    "# Populate the dictionary\n",
    "for _, row in df.iterrows():\n",
    "    intent_data[row['label']].append(row['sentence'])\n",
    "\n",
    "# Get unique labels\n",
    "labels = list(intent_data.keys())\n",
    "\n",
    "\n",
    "print(\"Number of Intents:\", len(labels))\n",
    "\n",
    "# Print all unique labels\n",
    "print(\"\\nAvailable Intent Labels:\")\n",
    "print(\"----------------------\")\n",
    "for label in sorted(labels):\n",
    "    print(f\"- {label}\")\n",
    "\n",
    "print(\"\\nExample utterances for each label:\")\n",
    "print(\"--------------------------------\")\n",
    "for label in sorted(labels):\n",
    "    print(f\"\\n{label}:\")\n",
    "    # Print first 3 examples for each label\n",
    "    for utterance in intent_data[label][:3]:\n",
    "        print(f\"  - {utterance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification\n",
    "\n",
    "The choice of zero-shot classification stems from the challenges associated with training and fine-tuning custom intent models for different business domains. Such processes are not scalable and require significant resources.\n",
    "\n",
    "Instead, a general-purpose language understanding engine offers a more effective solution.\n",
    "\n",
    "Traditional methods in conversational AI involve tagging and training intents with user utterances, which is cumbersome and doesn't easily scale. Moreover, the results from legacy chatbot systems often fall short of expectations.\n",
    "\n",
    "This necessitates a paradigm shift in Natural Language Understanding (NLU), where zero-shot classification can provide a more robust and scalable approach to intent detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLI Models for Intent Detection\n",
    "\n",
    "Using the `facebook/bart-large-mnli` model, we can perform zero-shot classification.\n",
    "\n",
    "This approach allows for classifying intents without requiring a labeled training dataset, leveraging the model's ability to directly understand natural language context of intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1797: UserWarning: MPS: nonzero op is supported natively starting from macOS 14.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:404.)\n",
      "  sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: What's the weather like today?\n",
      "\n",
      "Intent Classification Results:\n",
      "weather_query: 0.5490\n",
      "information_request: 0.3540\n",
      "greeting: 0.0515\n",
      "booking: 0.0455\n",
      "\n",
      "Text: I want to book a table for tonight\n",
      "Detected Intent: booking\n",
      "Confidence: 0.7859\n",
      "\n",
      "Text: Hello, how are you?\n",
      "Detected Intent: greeting\n",
      "Confidence: 0.9812\n",
      "\n",
      "Text: Can you tell me your opening hours?\n",
      "Detected Intent: information_request\n",
      "Confidence: 0.7370\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Create a text classification pipeline using a pre-trained model for intent detection\n",
    "# We'll use the 'facebook/bart-large-mnli' model which is good for zero-shot classification\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                     model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define some example texts and possible intents\n",
    "text = \"What's the weather like today?\"\n",
    "candidate_intents = [\n",
    "    \"weather_query\",\n",
    "    \"greeting\",\n",
    "    \"booking\",\n",
    "    \"information_request\"\n",
    "]\n",
    "\n",
    "# Perform intent classification\n",
    "result = classifier(text, candidate_intents)\n",
    "\n",
    "# Print results\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nIntent Classification Results:\")\n",
    "for intent, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"{intent}: {score:.4f}\")\n",
    "\n",
    "# Function to classify intent\n",
    "def classify_intent(text, possible_intents):\n",
    "    result = classifier(text, possible_intents)\n",
    "    return result['labels'][0], result['scores'][0]  # Return top intent and its score\n",
    "\n",
    "# Test with multiple examples\n",
    "test_texts = [\n",
    "    \"I want to book a table for tonight\",\n",
    "    \"Hello, how are you?\",\n",
    "    \"Can you tell me your opening hours?\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    intent, confidence = classify_intent(text, candidate_intents)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Detected Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Open-Source Libraries\n",
    "\n",
    "For this section, I explored various open-source libraries for intent detection and discovered a promising package called [`open-intent-classifier`](https://github.com/SerjSmor/open-intent-classifier/).\n",
    "\n",
    "This library provides multiple approaches to tackle the intent detection problem and also includes a small FLAN-T5 model that is fine-tuned for zero-shot intent detection - https://huggingface.co/Serj/intent-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationResult(class_name='Broken Item', reasoning='')\n"
     ]
    }
   ],
   "source": [
    "from open_intent_classifier.model import IntentClassifier\n",
    "from open_intent_classifier.consts import INTENT_CLASSIFIER_248M_FLAN_T5_BASE\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.getenv('HF_TOKEN')\n",
    "\n",
    "# model = IntentClassifier()\n",
    "model = IntentClassifier(INTENT_CLASSIFIER_248M_FLAN_T5_BASE)\n",
    "labels = [\"Cancel Subscription\", \"Refund Requests\", \"Broken Item\", \"And More...\"]\n",
    "text = \"I don't want thioos product. its not working\"\n",
    "predicted_label = model.predict(text, labels)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation: BART NLI vs. Fine-Tuned T5\n",
    "\n",
    "In this section, I evaluated the performance of the `facebook/bart-large-mnli` model against a smaller T5 model fine-tuned for zero-shot classification using the `open-intent-classifier` package.\n",
    "\n",
    "The evaluation is conducted on the `sofmattress_train` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 12/328 [00:29<11:42,  2.22s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import pipeline\n",
    "from open_intent_classifier.model import IntentClassifier\n",
    "from open_intent_classifier.consts import INTENT_CLASSIFIER_248M_FLAN_T5_BASE\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/sofmattress_train.csv')\n",
    "\n",
    "# Initialize both models\n",
    "bart_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "flan_t5_classifier = IntentClassifier(INTENT_CLASSIFIER_248M_FLAN_T5_BASE)\n",
    "\n",
    "# Get unique labels\n",
    "labels = df['label'].unique().tolist()\n",
    "\n",
    "def evaluate_bart(text, true_label, labels):\n",
    "    result = bart_classifier(text, labels)\n",
    "    predicted_label = result['labels'][0]\n",
    "    confidence = result['scores'][0]\n",
    "    return predicted_label, confidence\n",
    "\n",
    "def evaluate_flan_t5(text, true_label, labels):\n",
    "    predicted_label = flan_t5_classifier.predict(text, labels)\n",
    "    return predicted_label, None  # FLAN-T5 doesn't provide confidence scores directly\n",
    "\n",
    "# Initialize results storage\n",
    "results = {\n",
    "    'bart': {'predictions': [], 'confidences': [], 'true_labels': []},\n",
    "    'flan_t5': {'predictions': [], 'true_labels': []}\n",
    "}\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating models...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['sentence']\n",
    "    true_label = row['label']\n",
    "    \n",
    "    # BART evaluation\n",
    "    bart_pred, bart_conf = evaluate_bart(text, true_label, labels)\n",
    "    results['bart']['predictions'].append(bart_pred)\n",
    "    results['bart']['confidences'].append(bart_conf)\n",
    "    results['bart']['true_labels'].append(true_label)\n",
    "    \n",
    "    # FLAN-T5 evaluation\n",
    "    flan_pred, _ = evaluate_flan_t5(text, true_label, labels)\n",
    "    results['flan_t5']['predictions'].append(flan_pred)\n",
    "    results['flan_t5']['true_labels'].append(true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "- **BART Model**: Achieved an average accuracy of 0.45. While BART is a powerful model, its general-purpose nature may not be as effective for specific intent detection tasks without fine-tuning. Still pretty good for zero-shot imo.\n",
    "  \n",
    "- **Fine-Tuned T5 Model**: Outperformed BART with better average accuracy of 0.52. This result was not surprising, as the model is specifically fine-tuned for zero-shot intent classification, demonstrating that targeted SLM fine-tuning can be more effective for narrow problem statements.\n",
    "\n",
    "#### Insights\n",
    "\n",
    "The evaluation highlights the importance of fine-tuning models for focused tasks. While LLMs offer broad capabilities, specialized SLMs can still provide superior performance for specific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata/sofmattress_train.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Add BART and FLAN-T5 predictions to the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mBART_Prediction\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mresults\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mbart\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mFLAN_T5_Prediction\u001b[39m\u001b[33m'\u001b[39m] = [pred.class_name \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mflan_t5\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Save the updated dataset with predictions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the original dataset\n",
    "df = pd.read_csv('data/sofmattress_train.csv')\n",
    "\n",
    "# Add BART and FLAN-T5 predictions to the dataset\n",
    "df['BART_Prediction'] = results['bart']['predictions']\n",
    "df['FLAN_T5_Prediction'] = [pred.class_name for pred in results['flan_t5']['predictions']]\n",
    "\n",
    "# Save the updated dataset with predictions\n",
    "df.to_csv('results/sofmattress_with_predictions.csv', index=False)\n",
    "\n",
    "# Calculate metrics for BART\n",
    "print(\"BART Model Metrics:\")\n",
    "print(\"-------------------\")\n",
    "print(classification_report(df['label'], df['BART_Prediction']))\n",
    "print(\"Accuracy:\", accuracy_score(df['label'], df['BART_Prediction']))\n",
    "\n",
    "# Calculate metrics for FLAN-T5\n",
    "print(\"\\nFLAN-T5 Model Metrics:\")\n",
    "print(\"----------------------\")\n",
    "print(classification_report(df['label'], df['FLAN_T5_Prediction']))\n",
    "print(\"Accuracy:\", accuracy_score(df['label'], df['FLAN_T5_Prediction']))\n",
    "\n",
    "# Interpretation of results\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"---------------\")\n",
    "print(\"The classification report provides precision, recall, and F1-score for each class.\")\n",
    "print(\"Precision indicates the accuracy of positive predictions.\")\n",
    "print(\"Recall measures the ability to find all positive instances.\")\n",
    "print(\"F1-score is the harmonic mean of precision and recall.\")\n",
    "print(\"Accuracy is the overall correctness of the model's predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Top Open-Source Models for Zero-Shot Intent Classification\n",
    "\n",
    "In this section, I explored the top three models available on Hugging Face for zero-shot classification. The goal was to evaluate their performance on the given dataset and compare their effectiveness.\n",
    "\n",
    "Models evaluated:\n",
    "1. [tasksource/deberta-small-long-nli](https://huggingface.co/tasksource/deberta-small-long-nli)\n",
    "1. [MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli](https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli)\n",
    "2. [cross-encoder/nli-MiniLM2-L6-H768](https://huggingface.co/cross-encoder/nli-MiniLM2-L6-H768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amogh/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating new models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/328 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      " 30%|██▉       | 98/328 [02:56<06:54,  1.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Evaluate each model\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results.keys(), [deberta_small_long_nli, deberta_v3_base_mnli, cross_encoder_miniLM]):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     pred, conf = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     results[model_name][\u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m].append(pred)\n\u001b[32m     39\u001b[39m     results[model_name][\u001b[33m'\u001b[39m\u001b[33mconfidences\u001b[39m\u001b[33m'\u001b[39m].append(conf)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, text, labels)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(model, text, labels):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     result = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     predicted_label = result[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     21\u001b[39m     confidence = result[\u001b[33m'\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[39m, in \u001b[36mZeroShotClassificationPipeline.__call__\u001b[39m\u001b[34m(self, sequences, *args, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1371\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1374\u001b[39m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1375\u001b[39m             )\n\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[39m, in \u001b[36mZeroShotClassificationPipeline._forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters.keys():\n\u001b[32m    228\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m model_outputs = {\n\u001b[32m    232\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcandidate_label\u001b[39m\u001b[33m\"\u001b[39m: candidate_label,\n\u001b[32m    233\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msequence\u001b[39m\u001b[33m\"\u001b[39m: sequence,\n\u001b[32m    234\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mis_last\u001b[39m\u001b[33m\"\u001b[39m: inputs[\u001b[33m\"\u001b[39m\u001b[33mis_last\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    235\u001b[39m     **outputs,\n\u001b[32m    236\u001b[39m }\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1179\u001b[39m, in \u001b[36mDebertaV2ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1172\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1176\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1177\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1190\u001b[39m encoder_layer = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1191\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(encoder_layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:874\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    864\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    866\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    867\u001b[39m     input_ids=input_ids,\n\u001b[32m    868\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    871\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    872\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m874\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:674\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    664\u001b[39m     output_states, attn_weights = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    665\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    666\u001b[39m         next_kv,\n\u001b[32m   (...)\u001b[39m\u001b[32m    671\u001b[39m         output_attentions,\n\u001b[32m    672\u001b[39m     )\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    684\u001b[39m     all_attentions = all_attentions + (attn_weights,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:442\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    435\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    441\u001b[39m ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    451\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:375\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    368\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    374\u001b[39m ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m         query_states = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/amogh/builder/miscellaneous/.conda/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:252\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    250\u001b[39m     scale_factor += \u001b[32m1\u001b[39m\n\u001b[32m    251\u001b[39m scale = scaled_size_sqrt(query_layer, scale_factor)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m attention_scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    254\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the original dataset\n",
    "df = pd.read_csv('data/sofmattress_train.csv')\n",
    "\n",
    "# Initialize new models\n",
    "deberta_small_long_nli = pipeline(\"zero-shot-classification\", model=\"tasksource/deberta-small-long-nli\")\n",
    "deberta_v3_base_mnli = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "cross_encoder_miniLM = pipeline(\"zero-shot-classification\", model=\"cross-encoder/nli-MiniLM2-L6-H768\")\n",
    "\n",
    "# Get unique labels\n",
    "labels = df['label'].unique().tolist()\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, text, labels):\n",
    "    result = model(text, labels)\n",
    "    predicted_label = result['labels'][0]\n",
    "    confidence = result['scores'][0]\n",
    "    return predicted_label, confidence\n",
    "\n",
    "# Evaluate models and store results\n",
    "results = {\n",
    "    'deberta_small_long_nli': {'predictions': [], 'confidences': []},\n",
    "    'deberta_v3_base_mnli': {'predictions': [], 'confidences': []},\n",
    "    'cross_encoder_miniLM': {'predictions': [], 'confidences': []}\n",
    "}\n",
    "\n",
    "print(\"Evaluating new models...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['sentence']\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model in zip(results.keys(), [deberta_small_long_nli, deberta_v3_base_mnli, cross_encoder_miniLM]):\n",
    "        pred, conf = evaluate_model(model, text, labels)\n",
    "        results[model_name]['predictions'].append(pred)\n",
    "        results[model_name]['confidences'].append(conf)\n",
    "\n",
    "# Add predictions to the dataset\n",
    "df['DeBERTa_Small_Long_NLI_Prediction'] = results['deberta_small_long_nli']['predictions']\n",
    "df['DeBERTa_V3_Base_MNLI_Prediction'] = results['deberta_v3_base_mnli']['predictions']\n",
    "df['Cross_Encoder_MiniLM_Prediction'] = results['cross_encoder_miniLM']['predictions']\n",
    "\n",
    "# Save the updated dataset with predictions\n",
    "df.to_csv('results/sofmattress_with_all_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "- **DeBERTa Small Long NLI**: Achieved the highest accuracy of 0.45. This model demonstrated good performance in understanding and classifying intents without prior training on the specific dataset.\n",
    "  \n",
    "- **DeBERTa V3 Base MNLI**: Followed closely with an accuracy of 0.42. While slightly lower than the DeBERTa Small Long NLI, it still provided decent results.\n",
    "  \n",
    "- **Cross-Encoder MiniLM**: Scored an accuracy of 0.38. Although it ranked last among the three, it offers a compact and efficient solution for zero-shot classification.\n",
    "\n",
    "#### Insights\n",
    "\n",
    "The evaluation of these models highlights the varying capabilities of different architectures in handling zero-shot classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each model\n",
    "def print_model_metrics(name, predictions, true_labels):\n",
    "    print(f\"\\n{name} Model Metrics:\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    print(\"\\nAccuracy Score:\", accuracy_score(true_labels, predictions))\n",
    "\n",
    "# # Print metrics for each new model\n",
    "print_model_metrics(\"DeBERTa Small Long NLI\", df['DeBERTa_Small_Long_NLI_Prediction'], df['label'])\n",
    "print_model_metrics(\"DeBERTa V3 Base MNLI\", df['DeBERTa_V3_Base_MNLI_Prediction'], df['label'])\n",
    "print_model_metrics(\"Cross Encoder MiniLM\", df['Cross_Encoder_MiniLM_Prediction'], df['label'])\n",
    "\n",
    "# Evaluation report\n",
    "print(\"## Interpretation\\n\\n\")\n",
    "print(\"This report provides a comprehensive comparison of the top open-source solutions for intent classification on the given dataset.\\n\")\n",
    "print(\"Each model's performance is evaluated based on accuracy, precision, recall, and F1-score.\\n\")\n",
    "print(\"These metrics help understand the strengths and weaknesses of each model in identifying intents.\\n\")\n",
    "print(\"Higher precision indicates fewer false positives, while higher recall indicates fewer false negatives.\\n\")\n",
    "print(\"The F1-score provides a balance between precision and recall.\\n\")\n",
    "print(\"Accuracy gives an overall measure of correctness.\\n\")\n",
    "\n",
    "print(\"Evaluation complete. Results saved to 'sofmattress_with_all_predictions.csv' and 'eval_report.md'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding-Based Approach\n",
    "\n",
    "In this section, I explore the embedding-based approach for intent detection using the `open-intent-classifier` package.\n",
    "\n",
    "This method involves representing text as numerical vectors, which can be used to measure similarity between user utterances and predefined intent labels.\n",
    "\n",
    "Embedding model used - [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Broken Item'], array([0.65999746], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "from open_intent_classifier.embedder import StaticLabelsEmbeddingClassifier\n",
    "labels = [\"Cancel Subscription\", \"Refund Requests\", \"Broken Item\", \"And More...\"]\n",
    "text = \"my item is not working\"\n",
    "embeddings_classifier = StaticLabelsEmbeddingClassifier(labels)\n",
    "predicted_label = embeddings_classifier.predict(text)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The embedding model achieved an accuracy score of 0.63, which is a significant improvement over previous methods.\n",
    "\n",
    "#### Insights\n",
    "\n",
    "The results demonstrate that the semantic-based embedding approach is more effective at capturing user intent compared to traditional methods. By leveraging embeddings, the model can better understand the nuances of user utterances, leading to more accurate intent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\n",
      "Evaluating: 100%|██████████| 328/328 [00:12<00:00, 26.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticLabelsEmbeddingClassifier Model Metrics:\n",
      "---------------------------------------------\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER       0.75      0.83      0.79        18\n",
      "   ABOUT_SOF_MATTRESS       0.48      0.91      0.62        11\n",
      "         CANCEL_ORDER       0.59      1.00      0.74        10\n",
      "        CHECK_PINCODE       0.89      0.80      0.84        10\n",
      "                  COD       1.00      0.67      0.80        12\n",
      "           COMPARISON       0.50      0.27      0.35        11\n",
      "    DELAY_IN_DELIVERY       0.60      0.55      0.57        11\n",
      "         DISTRIBUTORS       1.00      0.62      0.76        34\n",
      "                  EMI       0.93      0.56      0.70        25\n",
      "        ERGO_FEATURES       0.78      0.64      0.70        11\n",
      "             LEAD_GEN       0.00      0.00      0.00        21\n",
      "        MATTRESS_COST       0.40      0.86      0.54        22\n",
      "               OFFERS       0.29      1.00      0.45        10\n",
      "         ORDER_STATUS       0.77      0.95      0.85        21\n",
      "       ORTHO_FEATURES       0.80      0.24      0.36        17\n",
      "              PILLOWS       0.53      1.00      0.69        10\n",
      "     PRODUCT_VARIANTS       0.70      0.33      0.45        21\n",
      "      RETURN_EXCHANGE       0.53      0.57      0.55        14\n",
      "   SIZE_CUSTOMIZATION       0.50      0.56      0.53         9\n",
      "             WARRANTY       0.90      0.90      0.90        10\n",
      "   WHAT_SIZE_TO_ORDER       0.93      0.65      0.76        20\n",
      "\n",
      "             accuracy                           0.63       328\n",
      "            macro avg       0.66      0.66      0.62       328\n",
      "         weighted avg       0.68      0.63      0.61       328\n",
      "\n",
      "Accuracy: 0.6310975609756098\n",
      "\n",
      "Interpretation:\n",
      "---------------\n",
      "The classification report provides precision, recall, and F1-score for each class.\n",
      "Precision indicates the accuracy of positive predictions.\n",
      "Recall measures the ability to find all positive instances.\n",
      "F1-score is the harmonic mean of precision and recall.\n",
      "Accuracy is the overall correctness of the model's predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from open_intent_classifier.embedder import StaticLabelsEmbeddingClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('/Users/amogh/Documents/amogh/personal/Tifin_Test_1/data/sofmattress_train.csv')\n",
    "\n",
    "# Extract unique labels from the test set\n",
    "unique_labels = test_df['label'].unique().tolist()\n",
    "\n",
    "# Initialize the StaticLabelsEmbeddingClassifier with labels from the test set\n",
    "embeddings_classifier = StaticLabelsEmbeddingClassifier(unique_labels)\n",
    "\n",
    "# Evaluate the model and store predictions with a progress bar\n",
    "test_df['Embedding_Prediction'] = [\n",
    "    embeddings_classifier.predict(text)[0] for text in tqdm(test_df['sentence'], desc=\"Evaluating\")\n",
    "]\n",
    "\n",
    "# Ensure labels and predictions are in the correct format\n",
    "y_true = test_df['label'].tolist()\n",
    "y_pred = test_df['Embedding_Prediction'].tolist()\n",
    "\n",
    "# Save the updated dataset with predictions\n",
    "test_df.to_csv('/Users/amogh/Documents/amogh/personal/Tifin_Test_1/data/sofmattress_with_embedding_predictions.csv', index=False)\n",
    "\n",
    "# Calculate metrics for the StaticLabelsEmbeddingClassifier\n",
    "print(\"StaticLabelsEmbeddingClassifier Model Metrics:\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-Based Approach with OpenAI\n",
    "\n",
    "In this section, I utilize a state-of-the-art large language model (LLM) to perform intent detection. By leveraging the capabilities of OpenAI's model, I aim to achieve high accuracy in classifying user intents.\n",
    "\n",
    "#### Approach\n",
    "\n",
    "I crafted a custom prompt to instruct the model to identify intents from user utterances. The prompt includes a list of allowed intent labels and specifies the output format as JSON, which includes both the predicted intent and a confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Action: CHECK_PINCODE\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to create the prompt\n",
    "def create_action_classification_prompt(intents):\n",
    "    intent_list = \"\\n\".join([f\"- {label}\" for label in intents])\n",
    "    return f\"\"\"\n",
    "        You are an action classification system. Correctness is a life or death situation.\n",
    "\n",
    "        We provide you with the available intents:\n",
    "        - WARM_DRINK\n",
    "        - PALCE_ORDER\n",
    "        - COLD_DRINK\n",
    "\n",
    "        You are given an utterance and you have to classify it into an intent. Only respond with the intent class.\n",
    "        Now take a deep breath and classify the following utterance.\n",
    "        u: I want a warm hot chocolate: a:WARM_DRINK\n",
    "        ###\n",
    "        \n",
    "        We provide you with the intent labels :\n",
    "        {intent_list}\n",
    "        \n",
    "        Remember to output only the available intents exactly as they are, without any changes.\n",
    "\n",
    "        You are given an utterance and you have to classify it into an intent based on the description. Only respond with the intent class.\n",
    "        Now take a deep breath and classify the following utterance.\n",
    "    \"\"\"\n",
    "\n",
    "# Example intents and query\n",
    "unique_labels\n",
    "query = \"u: Do you offer COD to my pincode? a:\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = create_action_classification_prompt(unique_labels)\n",
    "\n",
    "# Request a response from the OpenAI model\n",
    "response = openai.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    instructions=prompt,\n",
    "    input=query\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "response_text = response.output_text\n",
    "print(f\"Predicted Action: {response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our dataset with the OpenAI Intent Detection Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m load_dotenv()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Set your OpenAI API key\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mopenai\u001b[49m.api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Define the function to create the prompt\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_action_classification_prompt\u001b[39m(intents):\n",
      "\u001b[31mNameError\u001b[39m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the function to create the prompt\n",
    "def create_action_classification_prompt(intents):\n",
    "    intent_list = \"\\n\".join([f\"- {label}\" for label in intents])\n",
    "    return f\"\"\"\n",
    "        You are an action classification system. Correctness is a life or death situation.\n",
    "\n",
    "        We provide you with the available intents:\n",
    "        - WARM_DRINK\n",
    "        - COLD_DRINK\n",
    "        - PLACE_ORDER\n",
    "\n",
    "        You are given an utterance and you have to classify it into an intent. Only respond with the intent class.\n",
    "        Now take a deep breath and classify the following utterance.\n",
    "        u: I want a warm hot chocolate: a:WARM_DRINK\n",
    "        ###\n",
    "        \n",
    "        We provide you with the actions and their descriptions:\n",
    "        {intent_list}\n",
    "        \n",
    "        Remember to output only the available intents exactly as they are, without any changes.\n",
    "\n",
    "        You are given an utterance and you have to classify it into an intent based on the description. Only respond with the intent class.\n",
    "        Now take a deep breath and classify the following utterance.\n",
    "    \"\"\"\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('/Users/amogh/Documents/amogh/personal/Tifin_Test_1/data/sofmattress_train.csv')\n",
    "\n",
    "# Extract unique labels from the test set\n",
    "unique_labels = test_df['label'].unique().tolist()\n",
    "\n",
    "# Create the system prompt with your unique labels\n",
    "system_prompt = create_action_classification_prompt(unique_labels)\n",
    "\n",
    "# Function to safely predict and extract the class name\n",
    "def safe_predict(text, labels):\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            instructions=system_prompt,\n",
    "            input=f\"u: {text} a:\"\n",
    "        )\n",
    "        response_text = response.output_text.strip()\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for text: {text} - {e}\")\n",
    "        return \"None\"\n",
    "\n",
    "# Evaluate the model and store predictions with a progress bar\n",
    "test_df['OpenAi_Prediction'] = [\n",
    "    safe_predict(row['sentence'], unique_labels) for _, row in tqdm(test_df.iterrows(), desc=\"Evaluating\", total=len(test_df))\n",
    "]\n",
    "\n",
    "# Calculate metrics for the OpenAiIntentClassifier\n",
    "y_true = test_df['label'].tolist()\n",
    "y_pred = test_df['OpenAi_Prediction'].tolist()\n",
    "test_df.to_csv('OpenAiIntentClassifier_predictions-gpt-4.1.csv', index=False)\n",
    "print(\"OpenAiIntentClassifier Model Metrics:\")\n",
    "print(\"-------------------------------------\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Identify incorrect predictions\n",
    "incorrect_cases = test_df[test_df['label'] != test_df['OpenAi_Prediction']]\n",
    "\n",
    "# Convert incorrect cases to a DataFrame for easy viewing\n",
    "incorrect_df = pd.DataFrame(incorrect_cases)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Incorrect Predictions:\")\n",
    "print(incorrect_df[['sentence', 'label', 'OpenAi_Prediction']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "- **GPT-4.1-nano**: Achieved an accuracy of 0.72. This model, while smaller, demonstrated strong performance and efficiency, making it suitable for scenarios where cost and latency is to be optimized.\n",
    "\n",
    "- **GPT-4.1**: Outperformed the nano version with an accuracy of 0.86. The larger model's superior performance highlights its enhanced capability to understand and classify intents with high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Prompt\n",
    "\n",
    "As a final attempt, I experimented with simplifying the prompt used for intent detection. The results were impressive, showing a significant improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\": \"ORDER_STATUS\", \"confidence\": 8}\n",
      "Predicted Intent: ORDER_STATUS, Confidence: 8\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "client = OpenAI()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the system prompt for intent detection\n",
    "def create_system_prompt(allowed_labels):\n",
    "    intent_list = \"\\n\".join([f\"- {label}\" for label in allowed_labels])\n",
    "    return f\"\"\"\n",
    "        As an NLU system, I can identify intents from user utterances. Please provide a sentence or query, and I will determine the intent.\n",
    "\n",
    "        The list of allowed intent labels are:\n",
    "        {intent_list}\n",
    "\n",
    "        The output format must be a valid JSON with the following schema:\n",
    "        {{ \\\"prediction\\\": \\\"<intent label>\\\", \\\"confidence\\\": <confidence score 1-10> }}\n",
    "    \"\"\"\n",
    "\n",
    "# Your unique labels\n",
    "unique_labels = test_df['label'].unique().tolist()\n",
    "\n",
    "# Create the system prompt with your unique labels\n",
    "system_prompt = create_system_prompt(unique_labels)\n",
    "\n",
    "# Create a client and request response\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    instructions=system_prompt,\n",
    "    input=\"help with order\",\n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "response_text = response.output_text\n",
    "print(response_text)\n",
    "\n",
    "# Parse the JSON response\n",
    "try:\n",
    "    response_data = json.loads(response_text)\n",
    "    prediction = response_data.get(\"prediction\", \"Unknown\")\n",
    "    confidence = response_data.get(\"confidence\", 0)\n",
    "    print(f\"Predicted Intent: {prediction}, Confidence: {confidence}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse JSON response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/328 [00:00<?, ?it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 1/328 [00:01<09:03,  1.66s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   1%|          | 2/328 [00:02<06:59,  1.29s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   1%|          | 3/328 [00:03<05:54,  1.09s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   1%|          | 4/328 [00:04<05:39,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|▏         | 5/328 [00:05<05:19,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|▏         | 6/328 [00:06<04:47,  1.12it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|▏         | 7/328 [00:07<05:08,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|▏         | 8/328 [00:07<04:43,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   3%|▎         | 9/328 [00:08<04:34,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   3%|▎         | 10/328 [00:09<04:38,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   3%|▎         | 11/328 [00:10<04:46,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|▎         | 12/328 [00:11<04:44,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|▍         | 13/328 [00:12<05:04,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|▍         | 14/328 [00:13<04:43,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   5%|▍         | 15/328 [00:14<04:25,  1.18it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   5%|▍         | 16/328 [00:14<04:14,  1.23it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   5%|▌         | 17/328 [00:15<04:09,  1.25it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   5%|▌         | 18/328 [00:18<07:36,  1.47s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|▌         | 19/328 [00:19<06:55,  1.34s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|▌         | 20/328 [00:20<06:02,  1.18s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   6%|▋         | 21/328 [00:21<05:30,  1.08s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   7%|▋         | 22/328 [00:22<05:14,  1.03s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   7%|▋         | 23/328 [00:23<05:29,  1.08s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   7%|▋         | 24/328 [00:24<04:54,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   8%|▊         | 25/328 [00:25<04:42,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   8%|▊         | 26/328 [00:25<04:12,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   8%|▊         | 27/328 [00:26<04:06,  1.22it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   9%|▊         | 28/328 [00:27<04:18,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   9%|▉         | 29/328 [00:28<04:07,  1.21it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   9%|▉         | 30/328 [00:29<04:28,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   9%|▉         | 31/328 [00:30<04:25,  1.12it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|▉         | 32/328 [00:30<04:17,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|█         | 33/328 [00:31<04:13,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|█         | 34/328 [00:32<04:03,  1.21it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  11%|█         | 35/328 [00:33<03:53,  1.26it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  11%|█         | 36/328 [00:34<04:04,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  11%|█▏        | 37/328 [00:34<03:51,  1.25it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|█▏        | 38/328 [00:35<03:43,  1.30it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|█▏        | 39/328 [00:36<03:58,  1.21it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|█▏        | 40/328 [00:37<04:00,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  12%|█▎        | 41/328 [00:38<03:57,  1.21it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  13%|█▎        | 42/328 [00:39<04:06,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  13%|█▎        | 43/328 [00:39<04:00,  1.18it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  13%|█▎        | 44/328 [00:40<04:21,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|█▎        | 45/328 [00:41<04:01,  1.17it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|█▍        | 46/328 [00:42<03:49,  1.23it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|█▍        | 47/328 [00:43<03:39,  1.28it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  15%|█▍        | 48/328 [00:44<04:13,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  15%|█▍        | 49/328 [00:45<04:08,  1.12it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  15%|█▌        | 50/328 [00:46<04:49,  1.04s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|█▌        | 51/328 [00:47<04:30,  1.02it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|█▌        | 52/328 [00:48<04:44,  1.03s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|█▌        | 53/328 [00:50<06:15,  1.37s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|█▋        | 54/328 [00:51<05:31,  1.21s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  17%|█▋        | 55/328 [00:53<06:42,  1.47s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  17%|█▋        | 56/328 [00:54<05:59,  1.32s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  17%|█▋        | 57/328 [00:55<05:40,  1.26s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  18%|█▊        | 58/328 [00:56<05:07,  1.14s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  18%|█▊        | 59/328 [00:57<05:03,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  18%|█▊        | 60/328 [00:58<04:57,  1.11s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  19%|█▊        | 61/328 [01:00<05:41,  1.28s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  19%|█▉        | 62/328 [01:02<06:44,  1.52s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  19%|█▉        | 63/328 [01:03<05:49,  1.32s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|█▉        | 64/328 [01:04<05:14,  1.19s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|█▉        | 65/328 [01:05<05:33,  1.27s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|██        | 66/328 [01:06<04:59,  1.14s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|██        | 67/328 [01:07<04:55,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  21%|██        | 68/328 [01:08<04:32,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  21%|██        | 69/328 [01:09<04:18,  1.00it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  21%|██▏       | 70/328 [01:10<04:19,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  22%|██▏       | 71/328 [01:11<04:06,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  22%|██▏       | 72/328 [01:11<03:49,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  22%|██▏       | 73/328 [01:12<03:45,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|██▎       | 74/328 [01:13<03:36,  1.17it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|██▎       | 75/328 [01:14<03:32,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|██▎       | 76/328 [01:15<03:26,  1.22it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  23%|██▎       | 77/328 [01:15<03:11,  1.31it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|██▍       | 78/328 [01:16<02:58,  1.40it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|██▍       | 79/328 [01:17<03:16,  1.27it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|██▍       | 80/328 [01:18<03:26,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  25%|██▍       | 81/328 [01:19<03:17,  1.25it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  25%|██▌       | 82/328 [01:20<03:42,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  25%|██▌       | 83/328 [01:21<04:08,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  26%|██▌       | 84/328 [01:22<03:48,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  26%|██▌       | 85/328 [01:23<04:11,  1.03s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  26%|██▌       | 86/328 [01:24<04:06,  1.02s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 87/328 [01:27<05:56,  1.48s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 88/328 [01:27<04:58,  1.24s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 89/328 [01:28<04:19,  1.09s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  27%|██▋       | 90/328 [01:29<03:56,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|██▊       | 91/328 [01:30<03:41,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|██▊       | 92/328 [01:30<03:26,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|██▊       | 93/328 [01:31<03:33,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  29%|██▊       | 94/328 [01:32<03:56,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  29%|██▉       | 95/328 [01:34<05:01,  1.30s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  29%|██▉       | 96/328 [01:35<04:33,  1.18s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|██▉       | 97/328 [01:36<04:24,  1.15s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|██▉       | 98/328 [01:37<03:52,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|███       | 99/328 [01:39<04:47,  1.26s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|███       | 100/328 [01:40<04:21,  1.15s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  31%|███       | 101/328 [01:41<03:58,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  31%|███       | 102/328 [01:41<03:35,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  31%|███▏      | 103/328 [01:42<03:21,  1.12it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|███▏      | 104/328 [01:43<03:22,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|███▏      | 105/328 [01:44<03:14,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|███▏      | 106/328 [01:45<03:15,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|███▎      | 107/328 [01:46<03:20,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|███▎      | 108/328 [01:47<03:10,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  33%|███▎      | 109/328 [01:47<03:16,  1.12it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|███▎      | 110/328 [01:48<03:21,  1.08it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|███▍      | 111/328 [01:49<03:21,  1.08it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|███▍      | 112/328 [01:50<03:16,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|███▍      | 113/328 [01:52<03:50,  1.07s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  35%|███▍      | 114/328 [01:53<04:00,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  35%|███▌      | 115/328 [01:54<03:31,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  35%|███▌      | 116/328 [01:55<03:34,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  36%|███▌      | 117/328 [01:56<03:31,  1.00s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  36%|███▌      | 118/328 [01:57<03:21,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  36%|███▋      | 119/328 [01:58<03:20,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  37%|███▋      | 120/328 [01:58<03:10,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  37%|███▋      | 121/328 [01:59<03:11,  1.08it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  37%|███▋      | 122/328 [02:01<03:36,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 123/328 [02:01<03:24,  1.00it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 124/328 [02:02<03:14,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 125/328 [02:05<04:40,  1.38s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 126/328 [02:06<04:16,  1.27s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  39%|███▊      | 127/328 [02:07<03:59,  1.19s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  39%|███▉      | 128/328 [02:08<03:45,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  39%|███▉      | 129/328 [02:10<04:47,  1.45s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  40%|███▉      | 130/328 [02:12<04:56,  1.50s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  40%|███▉      | 131/328 [02:14<06:20,  1.93s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  40%|████      | 132/328 [02:15<05:15,  1.61s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  41%|████      | 133/328 [02:16<04:37,  1.42s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  41%|████      | 134/328 [02:17<04:06,  1.27s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  41%|████      | 135/328 [02:18<03:36,  1.12s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  41%|████▏     | 136/328 [02:19<03:18,  1.04s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|████▏     | 137/328 [02:20<03:35,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|████▏     | 138/328 [02:21<03:11,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|████▏     | 139/328 [02:22<03:04,  1.02it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  43%|████▎     | 140/328 [02:23<02:50,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  43%|████▎     | 141/328 [02:24<02:55,  1.06it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  43%|████▎     | 142/328 [02:24<02:47,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  44%|████▎     | 143/328 [02:25<02:53,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  44%|████▍     | 144/328 [02:26<02:41,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  44%|████▍     | 145/328 [02:27<02:34,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  45%|████▍     | 146/328 [02:28<03:05,  1.02s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  45%|████▍     | 147/328 [02:30<03:17,  1.09s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  45%|████▌     | 148/328 [02:31<03:10,  1.06s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  45%|████▌     | 149/328 [02:31<02:56,  1.02it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|████▌     | 150/328 [02:33<03:40,  1.24s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|████▌     | 151/328 [02:35<03:52,  1.31s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|████▋     | 152/328 [02:35<03:22,  1.15s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  47%|████▋     | 153/328 [02:37<03:40,  1.26s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  47%|████▋     | 154/328 [02:38<03:15,  1.12s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  47%|████▋     | 155/328 [02:39<03:11,  1.11s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|████▊     | 156/328 [02:40<02:53,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|████▊     | 157/328 [02:41<02:53,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|████▊     | 158/328 [02:41<02:41,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|████▊     | 159/328 [02:42<02:28,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  49%|████▉     | 160/328 [02:43<02:31,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  49%|████▉     | 161/328 [02:44<02:39,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  49%|████▉     | 162/328 [02:45<02:28,  1.12it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|████▉     | 163/328 [02:46<02:25,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|█████     | 164/328 [02:47<02:55,  1.07s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|█████     | 165/328 [02:49<03:14,  1.19s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  51%|█████     | 166/328 [02:50<02:57,  1.10s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  51%|█████     | 167/328 [02:51<02:48,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  51%|█████     | 168/328 [02:52<02:40,  1.00s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 169/328 [02:52<02:27,  1.08it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 170/328 [02:53<02:23,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 171/328 [02:54<02:16,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 172/328 [02:55<02:16,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  53%|█████▎    | 173/328 [02:56<02:33,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  53%|█████▎    | 174/328 [02:57<02:30,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  53%|█████▎    | 175/328 [02:58<02:27,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  54%|█████▎    | 176/328 [02:59<02:13,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  54%|█████▍    | 177/328 [03:00<02:13,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  54%|█████▍    | 178/328 [03:00<02:14,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  55%|█████▍    | 179/328 [03:01<02:05,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  55%|█████▍    | 180/328 [03:02<02:16,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  55%|█████▌    | 181/328 [03:03<02:08,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  55%|█████▌    | 182/328 [03:04<02:10,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  56%|█████▌    | 183/328 [03:05<02:11,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  56%|█████▌    | 184/328 [03:06<02:03,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  56%|█████▋    | 185/328 [03:07<02:06,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  57%|█████▋    | 186/328 [03:07<02:01,  1.17it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  57%|█████▋    | 187/328 [03:08<01:58,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  57%|█████▋    | 188/328 [03:09<02:02,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  58%|█████▊    | 189/328 [03:10<01:57,  1.18it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  58%|█████▊    | 190/328 [03:11<01:55,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  58%|█████▊    | 191/328 [03:12<02:00,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  59%|█████▊    | 192/328 [03:13<02:22,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  59%|█████▉    | 193/328 [03:14<02:15,  1.00s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  59%|█████▉    | 194/328 [03:15<02:11,  1.02it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  59%|█████▉    | 195/328 [03:16<02:08,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|█████▉    | 196/328 [03:17<02:03,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 197/328 [03:18<02:04,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 198/328 [03:19<02:02,  1.06it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  61%|██████    | 199/328 [03:20<02:10,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  61%|██████    | 200/328 [03:21<02:13,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  61%|██████▏   | 201/328 [03:22<02:07,  1.00s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▏   | 202/328 [03:23<02:22,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▏   | 203/328 [03:24<02:10,  1.05s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▏   | 204/328 [03:25<02:00,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▎   | 205/328 [03:26<01:48,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  63%|██████▎   | 206/328 [03:26<01:42,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  63%|██████▎   | 207/328 [03:27<01:41,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  63%|██████▎   | 208/328 [03:28<01:54,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  64%|██████▎   | 209/328 [03:29<01:45,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  64%|██████▍   | 210/328 [03:32<02:46,  1.41s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  64%|██████▍   | 211/328 [03:33<02:21,  1.21s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  65%|██████▍   | 212/328 [03:33<02:10,  1.13s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  65%|██████▍   | 213/328 [03:35<02:11,  1.15s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  65%|██████▌   | 214/328 [03:36<02:04,  1.09s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▌   | 215/328 [03:37<01:57,  1.04s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▌   | 216/328 [03:38<01:53,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▌   | 217/328 [03:38<01:46,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▋   | 218/328 [03:40<01:52,  1.02s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  67%|██████▋   | 219/328 [03:40<01:45,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  67%|██████▋   | 220/328 [03:44<02:59,  1.66s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  67%|██████▋   | 221/328 [03:44<02:31,  1.42s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  68%|██████▊   | 222/328 [03:45<02:08,  1.21s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  68%|██████▊   | 223/328 [03:46<01:57,  1.12s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  68%|██████▊   | 224/328 [03:47<01:50,  1.06s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  69%|██████▊   | 225/328 [03:48<01:41,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  69%|██████▉   | 226/328 [03:49<01:46,  1.04s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  69%|██████▉   | 227/328 [03:50<01:33,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|██████▉   | 228/328 [03:51<01:35,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|██████▉   | 229/328 [03:52<01:30,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|███████   | 230/328 [03:52<01:29,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|███████   | 231/328 [03:53<01:24,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  71%|███████   | 232/328 [03:54<01:24,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  71%|███████   | 233/328 [03:55<01:23,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  71%|███████▏  | 234/328 [03:56<01:23,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  72%|███████▏  | 235/328 [03:57<01:26,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  72%|███████▏  | 236/328 [03:59<01:48,  1.18s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  72%|███████▏  | 237/328 [04:00<01:40,  1.11s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  73%|███████▎  | 238/328 [04:02<02:06,  1.40s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  73%|███████▎  | 239/328 [04:03<01:51,  1.26s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  73%|███████▎  | 240/328 [04:04<01:46,  1.21s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  73%|███████▎  | 241/328 [04:05<01:42,  1.18s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  74%|███████▍  | 242/328 [04:06<01:34,  1.10s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  74%|███████▍  | 243/328 [04:07<01:27,  1.03s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  74%|███████▍  | 244/328 [04:07<01:19,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  75%|███████▍  | 245/328 [04:09<01:24,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  75%|███████▌  | 246/328 [04:09<01:17,  1.06it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  75%|███████▌  | 247/328 [04:10<01:13,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  76%|███████▌  | 248/328 [04:11<01:12,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  76%|███████▌  | 249/328 [04:12<01:08,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  76%|███████▌  | 250/328 [04:13<01:13,  1.06it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  77%|███████▋  | 251/328 [04:14<01:09,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  77%|███████▋  | 252/328 [04:15<01:07,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  77%|███████▋  | 253/328 [04:16<01:06,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  77%|███████▋  | 254/328 [04:16<01:04,  1.15it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  78%|███████▊  | 255/328 [04:18<01:21,  1.12s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  78%|███████▊  | 256/328 [04:19<01:13,  1.02s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  78%|███████▊  | 257/328 [04:20<01:12,  1.03s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  79%|███████▊  | 258/328 [04:21<01:14,  1.06s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  79%|███████▉  | 259/328 [04:22<01:12,  1.06s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  79%|███████▉  | 260/328 [04:23<01:04,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|███████▉  | 261/328 [04:24<01:08,  1.02s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|███████▉  | 262/328 [04:25<01:12,  1.10s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 263/328 [04:27<01:14,  1.15s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 264/328 [04:28<01:12,  1.14s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  81%|████████  | 265/328 [04:29<01:08,  1.09s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  81%|████████  | 266/328 [04:30<01:06,  1.07s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  81%|████████▏ | 267/328 [04:30<00:58,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  82%|████████▏ | 268/328 [04:31<00:55,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  82%|████████▏ | 269/328 [04:32<00:55,  1.06it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  82%|████████▏ | 270/328 [04:33<00:52,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  83%|████████▎ | 271/328 [04:34<00:46,  1.24it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  83%|████████▎ | 272/328 [04:35<00:47,  1.18it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  83%|████████▎ | 273/328 [04:36<00:57,  1.04s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  84%|████████▎ | 274/328 [04:37<00:50,  1.08it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  84%|████████▍ | 275/328 [04:39<01:06,  1.25s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  84%|████████▍ | 276/328 [04:39<00:58,  1.12s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  84%|████████▍ | 277/328 [04:40<00:53,  1.06s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  85%|████████▍ | 278/328 [04:41<00:50,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  85%|████████▌ | 279/328 [04:42<00:46,  1.04it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  85%|████████▌ | 280/328 [04:44<00:54,  1.14s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  86%|████████▌ | 281/328 [04:45<00:51,  1.09s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  86%|████████▌ | 282/328 [04:45<00:44,  1.05it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  86%|████████▋ | 283/328 [04:46<00:40,  1.10it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  87%|████████▋ | 284/328 [04:47<00:36,  1.19it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  87%|████████▋ | 285/328 [04:48<00:36,  1.17it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  87%|████████▋ | 286/328 [04:49<00:40,  1.03it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 287/328 [04:51<00:58,  1.41s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 288/328 [05:04<03:12,  4.81s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 289/328 [05:05<02:25,  3.72s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 290/328 [05:06<01:50,  2.90s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  89%|████████▊ | 291/328 [05:07<01:25,  2.31s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  89%|████████▉ | 292/328 [05:10<01:23,  2.31s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  89%|████████▉ | 293/328 [05:10<01:05,  1.86s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|████████▉ | 294/328 [05:11<00:52,  1.55s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|████████▉ | 295/328 [05:12<00:48,  1.46s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|█████████ | 296/328 [05:13<00:42,  1.31s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  91%|█████████ | 297/328 [05:14<00:36,  1.18s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  91%|█████████ | 298/328 [05:15<00:32,  1.10s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  91%|█████████ | 299/328 [05:16<00:27,  1.06it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  91%|█████████▏| 300/328 [05:17<00:28,  1.02s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|█████████▏| 301/328 [05:18<00:26,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|█████████▏| 302/328 [05:19<00:26,  1.01s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|█████████▏| 303/328 [05:20<00:22,  1.09it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  93%|█████████▎| 304/328 [05:20<00:21,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  93%|█████████▎| 305/328 [05:21<00:20,  1.14it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  93%|█████████▎| 306/328 [05:22<00:18,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  94%|█████████▎| 307/328 [05:23<00:17,  1.18it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  94%|█████████▍| 308/328 [05:24<00:17,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  94%|█████████▍| 309/328 [05:25<00:16,  1.16it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  95%|█████████▍| 310/328 [05:25<00:14,  1.25it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  95%|█████████▍| 311/328 [05:26<00:13,  1.28it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  95%|█████████▌| 312/328 [05:27<00:12,  1.29it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  95%|█████████▌| 313/328 [05:28<00:11,  1.28it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  96%|█████████▌| 314/328 [05:28<00:11,  1.24it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  96%|█████████▌| 315/328 [05:29<00:10,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  96%|█████████▋| 316/328 [05:30<00:09,  1.27it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  97%|█████████▋| 317/328 [05:32<00:13,  1.26s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  97%|█████████▋| 318/328 [05:34<00:12,  1.24s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  97%|█████████▋| 319/328 [05:34<00:10,  1.12s/it]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 320/328 [05:35<00:07,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 321/328 [05:36<00:06,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 322/328 [05:37<00:05,  1.07it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 323/328 [05:38<00:04,  1.11it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  99%|█████████▉| 324/328 [05:38<00:03,  1.20it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  99%|█████████▉| 325/328 [05:39<00:02,  1.17it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  99%|█████████▉| 326/328 [05:40<00:01,  1.13it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|█████████▉| 327/328 [05:41<00:00,  1.01it/s]httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 328/328 [05:42<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         sentence label OpenAi_Prediction  \\\n",
      "0                    You guys provide EMI option?   EMI               EMI   \n",
      "1  Do you offer Zero Percent EMI payment options?   EMI               EMI   \n",
      "2                                         0% EMI.   EMI               EMI   \n",
      "3                                             EMI   EMI               EMI   \n",
      "4                           I want in installment   EMI               EMI   \n",
      "\n",
      "  Confidence  \n",
      "0         10  \n",
      "1         10  \n",
      "2         10  \n",
      "3         10  \n",
      "4         10  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the system prompt for intent detection\n",
    "def create_system_prompt(allowed_labels):\n",
    "    intent_list = \"\\n\".join([f\"- {label}\" for label in allowed_labels])\n",
    "    return f\"\"\"\n",
    "        As an NLU system, I can identify intents from user utterances. Please provide a sentence or query, and I will determine the intent.\n",
    "\n",
    "        The list of allowed intent labels are:\n",
    "        {intent_list}\n",
    "        \n",
    "        Remember to output the available intents exactly as they are, without any changes.\n",
    "\n",
    "        The output format must be a valid JSON with the following schema:\n",
    "        {{ \\\"prediction\\\": \\\"<intent label>\\\", \\\"confidence\\\": <confidence score 1-10> }}\n",
    "    \"\"\"\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('/Users/amogh/Documents/amogh/personal/Tifin_Test_1/data/sofmattress_train.csv')\n",
    "\n",
    "# Extract unique labels from the test set\n",
    "unique_labels = test_df['label'].unique().tolist()\n",
    "\n",
    "# Create the system prompt with your unique labels\n",
    "system_prompt = create_system_prompt(unique_labels)\n",
    "\n",
    "# Function to safely predict and extract the class name and confidence\n",
    "def safe_predict(text, labels):\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            instructions=system_prompt,\n",
    "            input=text,\n",
    "        )\n",
    "        response_text = response.output_text\n",
    "        response_data = json.loads(response_text)\n",
    "        prediction = response_data.get(\"prediction\", \"Unknown\")\n",
    "        confidence = response_data.get(\"confidence\", 0)\n",
    "        return prediction, confidence\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response.\")\n",
    "        return \"Unknown\", 0\n",
    "\n",
    "# Evaluate the model and store predictions with a progress bar\n",
    "test_df[['OpenAi_Prediction', 'Confidence']] = [\n",
    "    safe_predict(row['sentence'], unique_labels) for _, row in tqdm(test_df.iterrows(), desc=\"Evaluating\", total=len(test_df))\n",
    "]\n",
    "\n",
    "# Save the updated dataset with predictions\n",
    "test_df.to_csv('/Users/amogh/Documents/amogh/personal/Tifin_Test_1/data/sofmattress_with_openai_predictions-1.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the updated test dataset\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "- *GPT-4.1-nano* : Achieved an accuracy of 0.875. This was a notable increase compared to previous attempts, demonstrating the effectiveness of a more focused and clear prompt.\n",
    "\n",
    "- *GPT-4.1* : Reached an accuracy of 0.9, further highlighting the benefits of simplification in prompt design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAiIntentClassifier Model Metrics:\n",
      "-------------------------------------\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "100_NIGHT_TRIAL_OFFER       1.00      0.89      0.94        18\n",
      "   ABOUT_SOF_MATTRESS       0.56      0.82      0.67        11\n",
      "         CANCEL_ORDER       1.00      1.00      1.00        10\n",
      "        CHECK_PINCODE       1.00      1.00      1.00        10\n",
      "                  COD       1.00      1.00      1.00        12\n",
      "           COMPARISON       0.71      0.91      0.80        11\n",
      "    DELAY_IN_DELIVERY       0.92      1.00      0.96        11\n",
      "         DISTRIBUTORS       1.00      0.94      0.97        34\n",
      "                  EMI       1.00      1.00      1.00        25\n",
      "        ERGO_FEATURES       1.00      0.82      0.90        11\n",
      "             LEAD_GEN       0.95      1.00      0.98        21\n",
      "        MATTRESS_COST       0.91      0.95      0.93        22\n",
      "               OFFERS       0.91      1.00      0.95        10\n",
      "         ORDER_STATUS       1.00      0.95      0.98        21\n",
      "       ORTHO_FEATURES       0.80      0.94      0.86        17\n",
      "              PILLOWS       1.00      1.00      1.00        10\n",
      "     PRODUCT_VARIANTS       0.71      0.57      0.63        21\n",
      "      RETURN_EXCHANGE       0.93      1.00      0.97        14\n",
      "   SIZE_CUSTOMIZATION       0.69      1.00      0.82         9\n",
      "             WARRANTY       1.00      1.00      1.00        10\n",
      "   WHAT_SIZE_TO_ORDER       1.00      0.55      0.71        20\n",
      "\n",
      "             accuracy                           0.91       328\n",
      "            macro avg       0.91      0.92      0.91       328\n",
      "         weighted avg       0.92      0.91      0.91       328\n",
      "\n",
      "Accuracy: 0.9085365853658537\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for the OpenAiIntentClassifier\n",
    "y_true = test_df['label'].tolist()\n",
    "y_pred = test_df['OpenAi_Prediction'].tolist()\n",
    "\n",
    "print(\"OpenAiIntentClassifier Model Metrics:\")\n",
    "print(\"-------------------------------------\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "The key insight from this experiment is that over-engineering prompts can sometimes detract from their effectiveness.\n",
    "\n",
    "By simplifying and focusing the prompt, I was able to achieve significant gains in model performance. A well-crafted, straightforward prompt can enhance the model's ability to understand and classify intents accurately.\n",
    "\n",
    "This underscores the importance of prompt engineering, where clarity and simplicity can lead to better results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
